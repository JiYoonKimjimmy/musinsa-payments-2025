# 09. 향후 개선 사항

## 1. 개요

현재 포인트 시스템은 단일 서버 환경을 기준으로 JPA 비관적 락을 통해 동시성을 제어하고 있습니다.
향후 시스템 확장 및 성능 개선을 위해 고려해야 할 개선 사항들을 정리합니다.

---

## 2. 분산 락 (Distributed Lock) 도입

### 2.1 배경 및 필요성

#### 현재 상황
- **단일 서버 환경**: JPA 비관적 락으로 동시성 제어
- **데이터베이스 락**: `SELECT ... FOR UPDATE`를 통한 행 단위 락
- **제한 사항**: 다중 서버 환경에서는 동일 회원의 요청이 여러 서버로 분산될 경우 동시성 제어 불가

#### 분산 락이 필요한 시점
1. **서버 스케일 아웃**: 트래픽 증가로 인한 다중 서버 운영 시
2. **마이크로서비스 아키텍처**: 포인트 서비스가 여러 인스턴스로 분리 운영될 때
3. **성능 개선**: 데이터베이스 락 경합을 줄이고 애플리케이션 레벨에서 제어

#### 기대 효과
- **다중 서버 환경 지원**: 여러 서버에서 동일 회원의 포인트 작업 순차 처리
- **데이터베이스 부하 감소**: DB 락 대신 애플리케이션 레벨 락 사용
- **성능 향상**: 락 획득/해제가 빠른 인메모리 기반 락 활용

---

### 2.2 분산 락 개요

#### 분산 락이란?
여러 서버에서 공유 자원에 대한 접근을 제어하기 위해 외부 저장소(Redis, Zookeeper 등)를 활용하여
락을 획득하고 해제하는 메커니즘입니다.

#### 분산 락 vs 비관적 락

| 구분 | 비관적 락 (현재) | 분산 락 (개선안) |
|------|-----------------|-----------------|
| 적용 범위 | 단일 서버 | 다중 서버 |
| 락 저장소 | 데이터베이스 | Redis (인메모리) |
| 성능 | DB 락 경합 시 느림 | 빠른 락 획득/해제 |
| 복잡도 | 낮음 | 중간 (Redis 인프라 필요) |
| 락 단위 | 행(Row) 단위 | 논리적 키 단위 (회원 ID 등) |

---

### 2.3 기술 스택

#### Redis + Redisson 조합 (권장)

**Redis**
- 인메모리 데이터 저장소
- 빠른 읽기/쓰기 성능
- TTL(Time To Live) 지원으로 락 자동 해제

**Redisson**
- Redis 기반 분산 락 라이브러리
- 공정 락(Fair Lock), 읽기/쓰기 락 등 다양한 락 지원
- 자동 락 갱신, 타임아웃 처리 등 편리한 기능 제공

#### 대안 기술

| 기술 | 장점 | 단점 |
|------|------|------|
| **Redisson** | 다양한 락 지원, 자동 갱신 | Redis 인프라 필요 |
| **Lettuce + Lua Script** | 가볍고 커스터마이징 가능 | 직접 구현 필요 |
| **Zookeeper** | 강력한 분산 코디네이션 | 복잡도 높음, 오버스펙 |

---

### 2.4 구현 방안

#### 2.4.1 의존성 추가

**build.gradle.kts**
```kotlin
dependencies {
    // 기존 의존성...

    // Redis & Redisson
    implementation("org.springframework.boot:spring-boot-starter-data-redis")
    implementation("org.redisson:redisson-spring-boot-starter:3.35.0")
}
```

#### 2.4.2 Redis 설정

**application.yml**
```yaml
spring:
  # 기존 설정...

  # Redis 설정
  data:
    redis:
      host: localhost
      port: 6379
      password: # Redis 비밀번호 (운영 환경)
      timeout: 3000ms
      lettuce:
        pool:
          max-active: 10
          max-idle: 10
          min-idle: 2

# Redisson 설정
redisson:
  config: |
    singleServerConfig:
      address: "redis://localhost:6379"
      connectionPoolSize: 10
      connectionMinimumIdleSize: 2
      timeout: 3000
```

#### 2.4.3 Redisson 설정 클래스

**RedissonConfig.kt**
```kotlin
package com.musinsa.payments.common.config

import org.redisson.Redisson
import org.redisson.api.RedissonClient
import org.redisson.config.Config
import org.springframework.beans.factory.annotation.Value
import org.springframework.context.annotation.Bean
import org.springframework.context.annotation.Configuration

@Configuration
class RedissonConfig(
    @Value("\${spring.data.redis.host}") private val redisHost: String,
    @Value("\${spring.data.redis.port}") private val redisPort: Int
) {

    @Bean
    fun redissonClient(): RedissonClient {
        val config = Config()
        config.useSingleServer()
            .setAddress("redis://$redisHost:$redisPort")
            .setConnectionPoolSize(10)
            .setConnectionMinimumIdleSize(2)
            .setTimeout(3000)

        return Redisson.create(config)
    }
}
```

#### 2.4.4 분산 락 AOP 구현 (선택적)

**DistributedLock 어노테이션**
```kotlin
package com.musinsa.payments.common.lock

import java.util.concurrent.TimeUnit

/**
 * 분산 락 어노테이션
 *
 * @property key 락 키 (SpEL 표현식 지원)
 * @property waitTime 락 획득 대기 시간 (초)
 * @property leaseTime 락 자동 해제 시간 (초)
 */
@Target(AnnotationTarget.FUNCTION)
@Retention(AnnotationRetention.RUNTIME)
annotation class DistributedLock(
    val key: String,
    val waitTime: Long = 5L,
    val leaseTime: Long = 10L,
    val timeUnit: TimeUnit = TimeUnit.SECONDS
)
```

**DistributedLockAspect**
```kotlin
package com.musinsa.payments.common.lock

import org.aspectj.lang.ProceedingJoinPoint
import org.aspectj.lang.annotation.Around
import org.aspectj.lang.annotation.Aspect
import org.aspectj.lang.reflect.MethodSignature
import org.redisson.api.RedissonClient
import org.slf4j.LoggerFactory
import org.springframework.expression.ExpressionParser
import org.springframework.expression.spel.standard.SpelExpressionParser
import org.springframework.expression.spel.support.StandardEvaluationContext
import org.springframework.stereotype.Component

@Aspect
@Component
class DistributedLockAspect(
    private val redissonClient: RedissonClient
) {

    private val logger = LoggerFactory.getLogger(javaClass)
    private val parser: ExpressionParser = SpelExpressionParser()

    @Around("@annotation(distributedLock)")
    fun around(joinPoint: ProceedingJoinPoint, distributedLock: DistributedLock): Any? {
        val lockKey = generateLockKey(joinPoint, distributedLock.key)
        val lock = redissonClient.getLock(lockKey)

        val acquired = lock.tryLock(
            distributedLock.waitTime,
            distributedLock.leaseTime,
            distributedLock.timeUnit
        )

        if (!acquired) {
            logger.warn("락 획득 실패: $lockKey")
            throw IllegalStateException("락을 획득할 수 없습니다. 잠시 후 다시 시도해주세요.")
        }

        return try {
            logger.debug("락 획득 성공: $lockKey")
            joinPoint.proceed()
        } finally {
            if (lock.isHeldByCurrentThread) {
                lock.unlock()
                logger.debug("락 해제 성공: $lockKey")
            }
        }
    }

    /**
     * SpEL을 사용하여 동적 락 키 생성
     */
    private fun generateLockKey(joinPoint: ProceedingJoinPoint, keyExpression: String): String {
        val signature = joinPoint.signature as MethodSignature
        val parameterNames = signature.parameterNames
        val args = joinPoint.args

        val context = StandardEvaluationContext()
        parameterNames.forEachIndexed { index, name ->
            context.setVariable(name, args[index])
        }

        val expression = parser.parseExpression(keyExpression)
        return expression.getValue(context, String::class.java)
            ?: throw IllegalArgumentException("락 키 생성 실패")
    }
}
```

#### 2.4.5 PointUsageService에 분산 락 적용

**Option 1: 어노테이션 방식 (권장)**
```kotlin
@Transactional(isolation = Isolation.READ_COMMITTED)
@Service
class PointUsageService(
    private val pointAccumulationPersistencePort: PointAccumulationPersistencePort,
    private val pointUsagePersistencePort: PointUsagePersistencePort,
    private val pointUsageDetailPersistencePort: PointUsageDetailPersistencePort,
    private val pointKeyGenerator: PointKeyGenerator,
    private val pointUsagePriorityService: PointUsagePriorityService
) : PointUsageUseCase {

    @DistributedLock(
        key = "'point:use:member:' + #memberId",
        waitTime = 5L,
        leaseTime = 10L
    )
    override fun use(
        memberId: Long,
        orderNumber: String,
        amount: Long
    ): PointUsage {
        // 기존 로직 유지
        // ...
    }
}
```

**Option 2: 직접 구현 방식**
```kotlin
@Transactional(isolation = Isolation.READ_COMMITTED)
@Service
class PointUsageService(
    private val pointAccumulationPersistencePort: PointAccumulationPersistencePort,
    private val pointUsagePersistencePort: PointUsagePersistencePort,
    private val pointUsageDetailPersistencePort: PointUsageDetailPersistencePort,
    private val pointKeyGenerator: PointKeyGenerator,
    private val pointUsagePriorityService: PointUsagePriorityService,
    private val redissonClient: RedissonClient  // 추가
) : PointUsageUseCase {

    override fun use(
        memberId: Long,
        orderNumber: String,
        amount: Long
    ): PointUsage {
        val lockKey = "point:use:member:$memberId"
        val lock = redissonClient.getLock(lockKey)

        val acquired = lock.tryLock(5, 10, TimeUnit.SECONDS)
        if (!acquired) {
            throw IllegalStateException("포인트 사용 중입니다. 잠시 후 다시 시도해주세요.")
        }

        return try {
            executeUse(memberId, orderNumber, amount)
        } finally {
            if (lock.isHeldByCurrentThread) {
                lock.unlock()
            }
        }
    }

    private fun executeUse(
        memberId: Long,
        orderNumber: String,
        amount: Long
    ): PointUsage {
        // 기존 로직 유지
        // ...
    }
}
```

#### 2.4.6 PointCancellationService에 분산 락 적용

```kotlin
@Transactional(isolation = Isolation.READ_COMMITTED)
@Service
class PointCancellationService(
    private val pointUsagePersistencePort: PointUsagePersistencePort,
    private val pointUsageDetailPersistencePort: PointUsageDetailPersistencePort,
    private val pointAccumulationPersistencePort: PointAccumulationPersistencePort,
    private val pointKeyGenerator: PointKeyGenerator,
    private val pointConfigPort: PointConfigPort
) : PointCancellationUseCase {

    @DistributedLock(
        key = "'point:cancel:usage:' + #pointKey",
        waitTime = 5L,
        leaseTime = 10L
    )
    override fun cancelUsage(
        pointKey: String,
        amount: Long?,
        reason: String?
    ): PointUsage {
        // 기존 로직 유지
        // ...
    }
}
```

---

### 2.5 락 키 전략

#### 2.5.1 락 키 설계 원칙

```
{서비스명}:{기능}:{엔티티}:{식별자}
```

#### 2.5.2 락 키 예시

| 기능 | 락 키 | 설명 |
|------|-------|------|
| 포인트 사용 | `point:use:member:{memberId}` | 회원별 포인트 사용 락 |
| 포인트 적립 | `point:accumulate:member:{memberId}` | 회원별 포인트 적립 락 |
| 사용 취소 | `point:cancel:usage:{pointKey}` | 사용 건별 취소 락 |
| 적립 취소 | `point:cancel:accumulation:{pointKey}` | 적립 건별 취소 락 |

#### 2.5.3 락 키 생성 규칙

1. **명확한 네이밍**: 락 키만 보고 어떤 작업인지 파악 가능
2. **충돌 방지**: 다른 기능과 락 키가 겹치지 않도록 설계
3. **일관된 형식**: 프로젝트 전체에서 동일한 형식 사용
4. **최소 범위**: 필요한 최소 범위만 락 설정 (회원별 > 전체)

---

### 2.6 구현 시 고려사항

#### 2.6.1 락 타임아웃 설정

```kotlin
@DistributedLock(
    key = "'point:use:member:' + #memberId",
    waitTime = 5L,      // 락 획득 대기 시간 (5초)
    leaseTime = 10L     // 락 자동 해제 시간 (10초)
)
```

**설정 기준:**
- `waitTime`: API 응답 시간 요구사항 고려 (일반적으로 3-5초)
- `leaseTime`: 최대 처리 시간 + 여유분 (일반적으로 10-30초)

#### 2.6.2 락 해제 실패 대응

**자동 만료 (TTL):**
- Redisson은 `leaseTime` 이후 자동으로 락 해제
- 애플리케이션 장애 시에도 락이 무한정 유지되지 않음

**락 갱신 (Watch Dog):**
- Redisson의 자동 락 갱신 기능 활용
- 작업이 길어질 경우 자동으로 `leaseTime` 연장

#### 2.6.3 재시도 전략

**Option 1: 즉시 실패 (권장)**
```kotlin
val acquired = lock.tryLock(0, 10, TimeUnit.SECONDS)  // waitTime = 0
if (!acquired) {
    throw IllegalStateException("다른 작업이 진행 중입니다. 잠시 후 다시 시도해주세요.")
}
```

**Option 2: 재시도**
```kotlin
val acquired = lock.tryLock(5, 10, TimeUnit.SECONDS)  // 5초 동안 재시도
```

**권장사항:**
- 사용자 요청: 즉시 실패 (응답 시간 보장)
- 백그라운드 작업: 재시도 허용

#### 2.6.4 Redis 장애 대응

**Circuit Breaker 패턴 적용**
```kotlin
@CircuitBreaker(name = "redis", fallbackMethod = "useFallback")
@DistributedLock(key = "'point:use:member:' + #memberId")
override fun use(memberId: Long, orderNumber: String, amount: Long): PointUsage {
    // 정상 처리
}

fun useFallback(memberId: Long, orderNumber: String, amount: Long, ex: Exception): PointUsage {
    logger.error("Redis 장애 발생, 비관적 락으로 폴백", ex)
    // 비관적 락으로 처리 (현재 구현)
    return executeUseWithPessimisticLock(memberId, orderNumber, amount)
}
```

#### 2.6.5 데드락 방지

**락 획득 순서 통일:**
```kotlin
// Good: 항상 동일한 순서로 락 획득
fun transferPoints(fromMemberId: Long, toMemberId: Long, amount: Long) {
    val (firstId, secondId) = if (fromMemberId < toMemberId) {
        fromMemberId to toMemberId
    } else {
        toMemberId to fromMemberId
    }

    val lock1 = redissonClient.getLock("point:member:$firstId")
    val lock2 = redissonClient.getLock("point:member:$secondId")

    lock1.lock()
    try {
        lock2.lock()
        try {
            // 포인트 이동 처리
        } finally {
            lock2.unlock()
        }
    } finally {
        lock1.unlock()
    }
}
```

#### 2.6.6 모니터링

**메트릭 수집:**
- 락 획득 성공/실패 횟수
- 락 대기 시간 분포
- 락 보유 시간 분포

**알람 설정:**
- 락 획득 실패율이 임계값 초과 시
- 락 대기 시간이 비정상적으로 긴 경우

---

### 2.7 테스트 전략

#### 2.7.1 단위 테스트

**Embedded Redis 활용**
```kotlin
@SpringBootTest
@Testcontainers
class PointUsageServiceDistributedLockTest {

    @Container
    private val redis = GenericContainer<Nothing>("redis:7-alpine").apply {
        withExposedPorts(6379)
    }

    @DynamicPropertySource
    fun properties(registry: DynamicPropertyRegistry) {
        registry.add("spring.data.redis.host") { redis.host }
        registry.add("spring.data.redis.port") { redis.getMappedPort(6379) }
    }

    // 테스트 코드...
}
```

#### 2.7.2 동시성 테스트

```kotlin
Given("분산 락이 적용된 포인트 사용 서비스가 있을 때") {
    When("다중 서버에서 동시에 포인트 사용을 시도하면") {
        val memberId = 1L
        val threadCount = 20
        val executorService = Executors.newFixedThreadPool(threadCount)
        val latch = CountDownLatch(threadCount)
        val results = ConcurrentHashMap<Int, Result<PointUsage>>()

        repeat(threadCount) { index ->
            executorService.submit {
                try {
                    val result = pointUsageService.use(memberId, "ORDER-$index", 1000L)
                    results[index] = Result.success(result)
                } catch (e: Exception) {
                    results[index] = Result.failure(e)
                } finally {
                    latch.countDown()
                }
            }
        }

        latch.await(30, TimeUnit.SECONDS)
        executorService.shutdown()

        Then("동시성 제어가 정상적으로 동작해야 한다") {
            val successCount = results.values.count { it.isSuccess }
            successCount shouldBe 10  // 10000원 / 1000원 = 10번 성공
        }
    }
}
```

#### 2.7.3 장애 시나리오 테스트

```kotlin
Given("Redis가 장애 상태일 때") {
    // Redis 컨테이너 중지
    redis.stop()

    When("포인트 사용을 시도하면") {
        Then("비관적 락으로 폴백되어야 한다") {
            // Circuit Breaker가 동작하여 비관적 락으로 처리
            val result = pointUsageService.use(1L, "ORDER-1", 1000L)
            result shouldNotBe null
        }
    }
}
```

---

### 2.8 성능 비교

#### 2.8.1 예상 성능 개선

| 지표 | 비관적 락 (현재) | 분산 락 (개선) | 개선율 |
|------|-----------------|---------------|--------|
| 락 획득 시간 | 5-10ms (DB 쿼리) | 1-2ms (Redis) | **80% 감소** |
| 락 해제 시간 | 3-5ms (DB 커밋) | <1ms (Redis) | **90% 감소** |
| 동시 처리량 | 100 TPS | 500 TPS | **400% 증가** |
| 데이터베이스 부하 | 높음 | 낮음 | **DB 부하 감소** |

**참고:** 위 수치는 예상치이며 실제 환경에 따라 달라질 수 있습니다.

#### 2.8.2 벤치마크 계획

**JMeter 시나리오:**
1. 동시 사용자: 100명
2. 요청 수: 10,000회
3. 측정 지표: 평균 응답 시간, TPS, 에러율

---

### 2.9 도입 로드맵

#### Phase 1: 인프라 구축 (1주)
- [ ] Redis 클러스터 구성 (개발/스테이징/운영)
- [ ] Redisson 의존성 추가 및 설정
- [ ] 모니터링 대시보드 구축

#### Phase 2: 개발 및 테스트 (2주)
- [ ] DistributedLock 어노테이션 및 AOP 구현
- [ ] PointUsageService 분산 락 적용
- [ ] PointCancellationService 분산 락 적용
- [ ] 단위 테스트 및 통합 테스트 작성

#### Phase 3: 성능 테스트 (1주)
- [ ] 부하 테스트 수행
- [ ] 비관적 락 vs 분산 락 성능 비교
- [ ] 튜닝 및 최적화

#### Phase 4: 카나리 배포 (1주)
- [ ] 스테이징 환경 배포 및 검증
- [ ] 운영 환경 일부 트래픽 적용 (10% → 50% → 100%)
- [ ] 모니터링 및 장애 대응

---

### 2.10 의사결정 기준

#### 분산 락 도입이 필요한 경우
- ✅ 서버가 2대 이상으로 스케일 아웃될 때
- ✅ 포인트 사용 트래픽이 1000 TPS 이상일 때
- ✅ 데이터베이스 락 경합으로 인한 성능 저하 발생 시

#### 비관적 락 유지가 적절한 경우
- ✅ 단일 서버 운영 (현재 상황)
- ✅ 트래픽이 낮은 경우 (100 TPS 이하)
- ✅ Redis 인프라 구축 비용이 부담스러운 경우

---

## 3. 기타 개선 사항

### 3.1 캐싱 전략

**Redis 캐시 도입**
- 회원 잔액 조회 캐싱
- 설정 값 캐싱
- TTL 기반 자동 갱신

### 3.2 비동기 처리

**이벤트 기반 아키텍처**
- 포인트 적립/사용 이벤트 발행
- 이메일/푸시 알림 비동기 처리
- Kafka/RabbitMQ를 통한 메시지 큐잉

### 3.3 읽기 성능 최적화

**CQRS 패턴 적용**
- 쓰기 모델과 읽기 모델 분리
- 읽기 전용 복제본(Read Replica) 활용
- 조회 API 응답 시간 개선

---

## 4. 결론

현재 구현된 비관적 락은 단일 서버 환경에서 충분히 효과적입니다.
하지만 시스템이 성장하고 트래픽이 증가하면 분산 락 도입을 검토해야 합니다.

**권장 사항:**
1. 현재는 비관적 락 유지 (Phase 1, 2)
2. 서버 스케일 아웃 필요 시 분산 락 도입 (Phase 3)
3. 성능 모니터링을 통해 도입 시점 결정

---

**다음 문서**: 없음 (마지막 문서)
