# 09. 향후 개선 사항

## 1. 개요

현재 포인트 시스템은 단일 서버 환경을 기준으로 JPA 비관적 락을 통해 동시성을 제어하고 있습니다.
향후 시스템 확장 및 성능 개선을 위해 고려해야 할 개선 사항들을 정리합니다.

---

## 2. 분산 락 (Distributed Lock) 도입

### 2.1 배경 및 필요성

#### 현재 상황
- **단일 서버 환경**: JPA 비관적 락으로 동시성 제어
- **데이터베이스 락**: `SELECT ... FOR UPDATE`를 통한 행 단위 락
- **제한 사항**: 다중 서버 환경에서는 동일 회원의 요청이 여러 서버로 분산될 경우 동시성 제어 불가

#### 분산 락이 필요한 시점
1. **서버 스케일 아웃**: 트래픽 증가로 인한 다중 서버 운영 시
2. **마이크로서비스 아키텍처**: 포인트 서비스가 여러 인스턴스로 분리 운영될 때
3. **성능 개선**: 데이터베이스 락 경합을 줄이고 애플리케이션 레벨에서 제어

#### 기대 효과
- **다중 서버 환경 지원**: 여러 서버에서 동일 회원의 포인트 작업 순차 처리
- **데이터베이스 부하 감소**: DB 락 대신 애플리케이션 레벨 락 사용
- **성능 향상**: 락 획득/해제가 빠른 인메모리 기반 락 활용

---

### 2.2 분산 락 개요

#### 분산 락이란?
여러 서버에서 공유 자원에 대한 접근을 제어하기 위해 외부 저장소(Redis, Zookeeper 등)를 활용하여
락을 획득하고 해제하는 메커니즘입니다.

#### 분산 락 vs 비관적 락

| 구분 | 비관적 락 (현재) | 분산 락 (개선안) |
|------|-----------------|-----------------|
| 적용 범위 | 단일 서버 | 다중 서버 |
| 락 저장소 | 데이터베이스 | Redis (인메모리) |
| 성능 | DB 락 경합 시 느림 | 빠른 락 획득/해제 |
| 복잡도 | 낮음 | 중간 (Redis 인프라 필요) |
| 락 단위 | 행(Row) 단위 | 논리적 키 단위 (회원 ID 등) |

---

### 2.3 기술 스택

#### Redis + Redisson 조합 (권장)

**Redis**
- 인메모리 데이터 저장소
- 빠른 읽기/쓰기 성능
- TTL(Time To Live) 지원으로 락 자동 해제

**Redisson**
- Redis 기반 분산 락 라이브러리
- 공정 락(Fair Lock), 읽기/쓰기 락 등 다양한 락 지원
- 자동 락 갱신, 타임아웃 처리 등 편리한 기능 제공

#### 대안 기술

| 기술 | 장점 | 단점 |
|------|------|------|
| **Redisson** | 다양한 락 지원, 자동 갱신 | Redis 인프라 필요 |
| **Lettuce + Lua Script** | 가볍고 커스터마이징 가능 | 직접 구현 필요 |
| **Zookeeper** | 강력한 분산 코디네이션 | 복잡도 높음, 오버스펙 |

---

### 2.4 구현 방안

#### 2.4.1 의존성 추가

**build.gradle.kts**
```kotlin
dependencies {
    // 기존 의존성...

    // Redis & Redisson
    implementation("org.springframework.boot:spring-boot-starter-data-redis")
    implementation("org.redisson:redisson-spring-boot-starter:3.35.0")
}
```

#### 2.4.2 Redis 설정

**application.yml**
```yaml
spring:
  # 기존 설정...

  # Redis 설정
  data:
    redis:
      host: localhost
      port: 6379
      password: # Redis 비밀번호 (운영 환경)
      timeout: 3000ms
      lettuce:
        pool:
          max-active: 10
          max-idle: 10
          min-idle: 2

# Redisson 설정
redisson:
  config: |
    singleServerConfig:
      address: "redis://localhost:6379"
      connectionPoolSize: 10
      connectionMinimumIdleSize: 2
      timeout: 3000
```

#### 2.4.3 Redisson 설정 클래스

**RedissonConfig.kt**
```kotlin
package com.musinsa.payments.common.config

import org.redisson.Redisson
import org.redisson.api.RedissonClient
import org.redisson.config.Config
import org.springframework.beans.factory.annotation.Value
import org.springframework.context.annotation.Bean
import org.springframework.context.annotation.Configuration

@Configuration
class RedissonConfig(
    @Value("\${spring.data.redis.host}") private val redisHost: String,
    @Value("\${spring.data.redis.port}") private val redisPort: Int
) {

    @Bean
    fun redissonClient(): RedissonClient {
        val config = Config()
        config.useSingleServer()
            .setAddress("redis://$redisHost:$redisPort")
            .setConnectionPoolSize(10)
            .setConnectionMinimumIdleSize(2)
            .setTimeout(3000)

        return Redisson.create(config)
    }
}
```

#### 2.4.4 분산 락 AOP 구현 (선택적)

**DistributedLock 어노테이션**
```kotlin
package com.musinsa.payments.common.lock

import java.util.concurrent.TimeUnit

/**
 * 분산 락 어노테이션
 *
 * @property key 락 키 (SpEL 표현식 지원)
 * @property waitTime 락 획득 대기 시간 (초)
 * @property leaseTime 락 자동 해제 시간 (초)
 */
@Target(AnnotationTarget.FUNCTION)
@Retention(AnnotationRetention.RUNTIME)
annotation class DistributedLock(
    val key: String,
    val waitTime: Long = 5L,
    val leaseTime: Long = 10L,
    val timeUnit: TimeUnit = TimeUnit.SECONDS
)
```

**DistributedLockAspect**
```kotlin
package com.musinsa.payments.common.lock

import org.aspectj.lang.ProceedingJoinPoint
import org.aspectj.lang.annotation.Around
import org.aspectj.lang.annotation.Aspect
import org.aspectj.lang.reflect.MethodSignature
import org.redisson.api.RedissonClient
import org.slf4j.LoggerFactory
import org.springframework.expression.ExpressionParser
import org.springframework.expression.spel.standard.SpelExpressionParser
import org.springframework.expression.spel.support.StandardEvaluationContext
import org.springframework.stereotype.Component

@Aspect
@Component
class DistributedLockAspect(
    private val redissonClient: RedissonClient
) {

    private val logger = LoggerFactory.getLogger(javaClass)
    private val parser: ExpressionParser = SpelExpressionParser()

    @Around("@annotation(distributedLock)")
    fun around(joinPoint: ProceedingJoinPoint, distributedLock: DistributedLock): Any? {
        val lockKey = generateLockKey(joinPoint, distributedLock.key)
        val lock = redissonClient.getLock(lockKey)

        val acquired = lock.tryLock(
            distributedLock.waitTime,
            distributedLock.leaseTime,
            distributedLock.timeUnit
        )

        if (!acquired) {
            logger.warn("락 획득 실패: $lockKey")
            throw IllegalStateException("락을 획득할 수 없습니다. 잠시 후 다시 시도해주세요.")
        }

        return try {
            logger.debug("락 획득 성공: $lockKey")
            joinPoint.proceed()
        } finally {
            if (lock.isHeldByCurrentThread) {
                lock.unlock()
                logger.debug("락 해제 성공: $lockKey")
            }
        }
    }

    /**
     * SpEL을 사용하여 동적 락 키 생성
     */
    private fun generateLockKey(joinPoint: ProceedingJoinPoint, keyExpression: String): String {
        val signature = joinPoint.signature as MethodSignature
        val parameterNames = signature.parameterNames
        val args = joinPoint.args

        val context = StandardEvaluationContext()
        parameterNames.forEachIndexed { index, name ->
            context.setVariable(name, args[index])
        }

        val expression = parser.parseExpression(keyExpression)
        return expression.getValue(context, String::class.java)
            ?: throw IllegalArgumentException("락 키 생성 실패")
    }
}
```

#### 2.4.5 PointUsageService에 분산 락 적용

**Option 1: 어노테이션 방식 (권장)**
```kotlin
@Transactional(isolation = Isolation.READ_COMMITTED)
@Service
class PointUsageService(
    private val pointAccumulationPersistencePort: PointAccumulationPersistencePort,
    private val pointUsagePersistencePort: PointUsagePersistencePort,
    private val pointUsageDetailPersistencePort: PointUsageDetailPersistencePort,
    private val pointKeyGenerator: PointKeyGenerator,
    private val pointUsagePriorityService: PointUsagePriorityService
) : PointUsageUseCase {

    @DistributedLock(
        key = "'point:use:member:' + #memberId",
        waitTime = 5L,
        leaseTime = 10L
    )
    override fun use(
        memberId: Long,
        orderNumber: String,
        amount: Long
    ): PointUsage {
        // 기존 로직 유지
        // ...
    }
}
```

**Option 2: 직접 구현 방식**
```kotlin
@Transactional(isolation = Isolation.READ_COMMITTED)
@Service
class PointUsageService(
    private val pointAccumulationPersistencePort: PointAccumulationPersistencePort,
    private val pointUsagePersistencePort: PointUsagePersistencePort,
    private val pointUsageDetailPersistencePort: PointUsageDetailPersistencePort,
    private val pointKeyGenerator: PointKeyGenerator,
    private val pointUsagePriorityService: PointUsagePriorityService,
    private val redissonClient: RedissonClient  // 추가
) : PointUsageUseCase {

    override fun use(
        memberId: Long,
        orderNumber: String,
        amount: Long
    ): PointUsage {
        val lockKey = "point:use:member:$memberId"
        val lock = redissonClient.getLock(lockKey)

        val acquired = lock.tryLock(5, 10, TimeUnit.SECONDS)
        if (!acquired) {
            throw IllegalStateException("포인트 사용 중입니다. 잠시 후 다시 시도해주세요.")
        }

        return try {
            executeUse(memberId, orderNumber, amount)
        } finally {
            if (lock.isHeldByCurrentThread) {
                lock.unlock()
            }
        }
    }

    private fun executeUse(
        memberId: Long,
        orderNumber: String,
        amount: Long
    ): PointUsage {
        // 기존 로직 유지
        // ...
    }
}
```

#### 2.4.6 PointCancellationService에 분산 락 적용

```kotlin
@Transactional(isolation = Isolation.READ_COMMITTED)
@Service
class PointCancellationService(
    private val pointUsagePersistencePort: PointUsagePersistencePort,
    private val pointUsageDetailPersistencePort: PointUsageDetailPersistencePort,
    private val pointAccumulationPersistencePort: PointAccumulationPersistencePort,
    private val pointKeyGenerator: PointKeyGenerator,
    private val pointConfigPort: PointConfigPort
) : PointCancellationUseCase {

    @DistributedLock(
        key = "'point:cancel:usage:' + #pointKey",
        waitTime = 5L,
        leaseTime = 10L
    )
    override fun cancelUsage(
        pointKey: String,
        amount: Long?,
        reason: String?
    ): PointUsage {
        // 기존 로직 유지
        // ...
    }
}
```

---

### 2.5 락 키 전략

#### 2.5.1 락 키 설계 원칙

```
{서비스명}:{기능}:{엔티티}:{식별자}
```

#### 2.5.2 락 키 예시

| 기능 | 락 키 | 설명 |
|------|-------|------|
| 포인트 사용 | `point:use:member:{memberId}` | 회원별 포인트 사용 락 |
| 포인트 적립 | `point:accumulate:member:{memberId}` | 회원별 포인트 적립 락 |
| 사용 취소 | `point:cancel:usage:{pointKey}` | 사용 건별 취소 락 |
| 적립 취소 | `point:cancel:accumulation:{pointKey}` | 적립 건별 취소 락 |

#### 2.5.3 락 키 생성 규칙

1. **명확한 네이밍**: 락 키만 보고 어떤 작업인지 파악 가능
2. **충돌 방지**: 다른 기능과 락 키가 겹치지 않도록 설계
3. **일관된 형식**: 프로젝트 전체에서 동일한 형식 사용
4. **최소 범위**: 필요한 최소 범위만 락 설정 (회원별 > 전체)

---

### 2.6 구현 시 고려사항

#### 2.6.1 락 타임아웃 설정

```kotlin
@DistributedLock(
    key = "'point:use:member:' + #memberId",
    waitTime = 5L,      // 락 획득 대기 시간 (5초)
    leaseTime = 10L     // 락 자동 해제 시간 (10초)
)
```

**설정 기준:**
- `waitTime`: API 응답 시간 요구사항 고려 (일반적으로 3-5초)
- `leaseTime`: 최대 처리 시간 + 여유분 (일반적으로 10-30초)

#### 2.6.2 락 해제 실패 대응

**자동 만료 (TTL):**
- Redisson은 `leaseTime` 이후 자동으로 락 해제
- 애플리케이션 장애 시에도 락이 무한정 유지되지 않음

**락 갱신 (Watch Dog):**
- Redisson의 자동 락 갱신 기능 활용
- 작업이 길어질 경우 자동으로 `leaseTime` 연장

#### 2.6.3 재시도 전략

**Option 1: 즉시 실패 (권장)**
```kotlin
val acquired = lock.tryLock(0, 10, TimeUnit.SECONDS)  // waitTime = 0
if (!acquired) {
    throw IllegalStateException("다른 작업이 진행 중입니다. 잠시 후 다시 시도해주세요.")
}
```

**Option 2: 재시도**
```kotlin
val acquired = lock.tryLock(5, 10, TimeUnit.SECONDS)  // 5초 동안 재시도
```

**권장사항:**
- 사용자 요청: 즉시 실패 (응답 시간 보장)
- 백그라운드 작업: 재시도 허용

#### 2.6.4 Redis 장애 대응

**Circuit Breaker 패턴 적용**
```kotlin
@CircuitBreaker(name = "redis", fallbackMethod = "useFallback")
@DistributedLock(key = "'point:use:member:' + #memberId")
override fun use(memberId: Long, orderNumber: String, amount: Long): PointUsage {
    // 정상 처리
}

fun useFallback(memberId: Long, orderNumber: String, amount: Long, ex: Exception): PointUsage {
    logger.error("Redis 장애 발생, 비관적 락으로 폴백", ex)
    // 비관적 락으로 처리 (현재 구현)
    return executeUseWithPessimisticLock(memberId, orderNumber, amount)
}
```

#### 2.6.5 데드락 방지

**락 획득 순서 통일:**
```kotlin
// Good: 항상 동일한 순서로 락 획득
fun transferPoints(fromMemberId: Long, toMemberId: Long, amount: Long) {
    val (firstId, secondId) = if (fromMemberId < toMemberId) {
        fromMemberId to toMemberId
    } else {
        toMemberId to fromMemberId
    }

    val lock1 = redissonClient.getLock("point:member:$firstId")
    val lock2 = redissonClient.getLock("point:member:$secondId")

    lock1.lock()
    try {
        lock2.lock()
        try {
            // 포인트 이동 처리
        } finally {
            lock2.unlock()
        }
    } finally {
        lock1.unlock()
    }
}
```

#### 2.6.6 모니터링

**메트릭 수집:**
- 락 획득 성공/실패 횟수
- 락 대기 시간 분포
- 락 보유 시간 분포

**알람 설정:**
- 락 획득 실패율이 임계값 초과 시
- 락 대기 시간이 비정상적으로 긴 경우

---

### 2.7 테스트 전략

#### 2.7.1 단위 테스트

**Embedded Redis 활용**
```kotlin
@SpringBootTest
@Testcontainers
class PointUsageServiceDistributedLockTest {

    @Container
    private val redis = GenericContainer<Nothing>("redis:7-alpine").apply {
        withExposedPorts(6379)
    }

    @DynamicPropertySource
    fun properties(registry: DynamicPropertyRegistry) {
        registry.add("spring.data.redis.host") { redis.host }
        registry.add("spring.data.redis.port") { redis.getMappedPort(6379) }
    }

    // 테스트 코드...
}
```

#### 2.7.2 동시성 테스트

```kotlin
Given("분산 락이 적용된 포인트 사용 서비스가 있을 때") {
    When("다중 서버에서 동시에 포인트 사용을 시도하면") {
        val memberId = 1L
        val threadCount = 20
        val executorService = Executors.newFixedThreadPool(threadCount)
        val latch = CountDownLatch(threadCount)
        val results = ConcurrentHashMap<Int, Result<PointUsage>>()

        repeat(threadCount) { index ->
            executorService.submit {
                try {
                    val result = pointUsageService.use(memberId, "ORDER-$index", 1000L)
                    results[index] = Result.success(result)
                } catch (e: Exception) {
                    results[index] = Result.failure(e)
                } finally {
                    latch.countDown()
                }
            }
        }

        latch.await(30, TimeUnit.SECONDS)
        executorService.shutdown()

        Then("동시성 제어가 정상적으로 동작해야 한다") {
            val successCount = results.values.count { it.isSuccess }
            successCount shouldBe 10  // 10000원 / 1000원 = 10번 성공
        }
    }
}
```

#### 2.7.3 장애 시나리오 테스트

```kotlin
Given("Redis가 장애 상태일 때") {
    // Redis 컨테이너 중지
    redis.stop()

    When("포인트 사용을 시도하면") {
        Then("비관적 락으로 폴백되어야 한다") {
            // Circuit Breaker가 동작하여 비관적 락으로 처리
            val result = pointUsageService.use(1L, "ORDER-1", 1000L)
            result shouldNotBe null
        }
    }
}
```

---

### 2.8 성능 비교

#### 2.8.1 예상 성능 개선

| 지표 | 비관적 락 (현재) | 분산 락 (개선) | 개선율 |
|------|-----------------|---------------|--------|
| 락 획득 시간 | 5-10ms (DB 쿼리) | 1-2ms (Redis) | **80% 감소** |
| 락 해제 시간 | 3-5ms (DB 커밋) | <1ms (Redis) | **90% 감소** |
| 동시 처리량 | 100 TPS | 500 TPS | **400% 증가** |
| 데이터베이스 부하 | 높음 | 낮음 | **DB 부하 감소** |

**참고:** 위 수치는 예상치이며 실제 환경에 따라 달라질 수 있습니다.

#### 2.8.2 벤치마크 계획

**JMeter 시나리오:**
1. 동시 사용자: 100명
2. 요청 수: 10,000회
3. 측정 지표: 평균 응답 시간, TPS, 에러율

---

### 2.9 도입 로드맵

#### Phase 1: 인프라 구축 (1주)
- [ ] Redis 클러스터 구성 (개발/스테이징/운영)
- [ ] Redisson 의존성 추가 및 설정
- [ ] 모니터링 대시보드 구축

#### Phase 2: 개발 및 테스트 (2주)
- [ ] DistributedLock 어노테이션 및 AOP 구현
- [ ] PointUsageService 분산 락 적용
- [ ] PointCancellationService 분산 락 적용
- [ ] 단위 테스트 및 통합 테스트 작성

#### Phase 3: 성능 테스트 (1주)
- [ ] 부하 테스트 수행
- [ ] 비관적 락 vs 분산 락 성능 비교
- [ ] 튜닝 및 최적화

#### Phase 4: 카나리 배포 (1주)
- [ ] 스테이징 환경 배포 및 검증
- [ ] 운영 환경 일부 트래픽 적용 (10% → 50% → 100%)
- [ ] 모니터링 및 장애 대응

---

### 2.10 의사결정 기준

#### 분산 락 도입이 필요한 경우
- ✅ 서버가 2대 이상으로 스케일 아웃될 때
- ✅ 포인트 사용 트래픽이 1000 TPS 이상일 때
- ✅ 데이터베이스 락 경합으로 인한 성능 저하 발생 시

#### 비관적 락 유지가 적절한 경우
- ✅ 단일 서버 운영 (현재 상황)
- ✅ 트래픽이 낮은 경우 (100 TPS 이하)
- ✅ Redis 인프라 구축 비용이 부담스러운 경우

---

## 3. Transactional Outbox Pattern

### 3.1 배경 및 필요성

#### 현재 상황

현재 포인트 시스템은 **Spring Event + @TransactionalEventListener**를 사용하여 이벤트 기반 아키텍처를 구현하고 있습니다:

```kotlin
// 현재 구현: Spring 인메모리 이벤트
@Async(AsyncConfig.POINT_EVENT_EXECUTOR)
@TransactionalEventListener(phase = TransactionPhase.AFTER_COMMIT)
@Transactional(propagation = Propagation.REQUIRES_NEW)
fun handleAccumulated(event: PointBalanceEvent.Accumulated) {
    // MemberPointBalance 캐시 테이블 업데이트
}
```

#### 현재 구현의 한계

| 구분 | 현재 상황 | 문제점 |
|------|----------|--------|
| 이벤트 저장 | 메모리 (인메모리) | 애플리케이션 재시작 시 이벤트 유실 |
| 전달 보장 | At-most-once | 이벤트 처리 실패 시 재시도 불가 |
| 분산 환경 | 단일 서버 | 다중 서버에서 이벤트 중복/유실 가능 |
| 장애 복구 | 불가 | 장애 발생 시 유실된 이벤트 추적 어려움 |

#### Outbox Pattern이 필요한 시점

1. **운영 환경 배포**: 이벤트 유실이 허용되지 않는 경우
2. **분산 환경**: 다중 서버 또는 마이크로서비스 아키텍처
3. **감사/추적 요구사항**: 모든 이벤트에 대한 이력 관리 필요
4. **장애 복구**: 시스템 장애 후 이벤트 재처리 필요

---

### 3.2 Transactional Outbox Pattern 개요

#### 패턴 설명

**Transactional Outbox Pattern**은 비즈니스 데이터와 이벤트를 **동일한 트랜잭션**으로 저장하여 
데이터 정합성과 이벤트 전달을 보장하는 패턴입니다.

```
┌─────────────────────────────────────────────────────────────────┐
│ 서비스 트랜잭션 (하나의 원자적 트랜잭션)                          │
│                                                                 │
│  1. 포인트 적립 저장  →  point_accumulation 테이블               │
│  2. 이벤트 저장       →  point_balance_outbox 테이블             │
│                                                                 │
│  [COMMIT] - 둘 다 성공하거나 둘 다 실패                          │
└─────────────────────────────────────────────────────────────────┘
                           ↓
┌─────────────────────────────────────────────────────────────────┐
│ 별도 프로세스 (폴링 방식 또는 CDC)                               │
│                                                                 │
│  1. Outbox 테이블에서 미처리 이벤트 조회                         │
│  2. 이벤트 처리 (MemberPointBalance 업데이트)                    │
│  3. Outbox 테이블에서 처리 완료 표시                             │
│  4. 실패 시 재시도 (최대 N회)                                    │
└─────────────────────────────────────────────────────────────────┘
```

#### 현재 구현 vs Outbox Pattern 비교

| 구분 | 현재 구현 (Spring Event) | Outbox Pattern |
|------|--------------------------|----------------|
| 이벤트 저장 | 메모리 | **DB (Outbox 테이블)** |
| 트랜잭션 원자성 | 별도 트랜잭션 | **동일 트랜잭션** |
| 이벤트 유실 | **가능** (앱 재시작 시) | **불가능** |
| 전달 보장 | At-most-once | **At-least-once** |
| 재시도 | 수동 구현 필요 | **내장 지원** |
| 순서 보장 | 보장 안됨 | **보장 가능** |
| 장애 복구 | 어려움 | **용이** |
| 구현 복잡도 | 낮음 | 중간 |

---

### 3.3 우리 프로젝트 적용 시 기대 효과

#### 3.3.1 이벤트 유실 방지

현재 `MemberPointBalance` 캐시 테이블은 비동기 이벤트로 업데이트됩니다.
Outbox Pattern 적용 시 이벤트가 DB에 저장되므로 **애플리케이션 재시작이나 장애 시에도 이벤트가 유실되지 않습니다**.

```kotlin
// Before: 이벤트 유실 가능
applicationEventPublisher.publishEvent(PointBalanceEvent.Accumulated(...))
// 애플리케이션 재시작 시 이벤트 유실!

// After: 이벤트 영속화
outboxRepository.save(PointBalanceOutbox(
    eventType = "ACCUMULATED",
    payload = jsonPayload,
    status = OutboxStatus.PENDING
))
// DB에 저장되므로 유실 불가
```

#### 3.3.2 At-least-once 전달 보장

| 시나리오 | 현재 구현 | Outbox Pattern |
|----------|----------|----------------|
| 이벤트 처리 성공 | ✅ 정상 | ✅ 정상 |
| 이벤트 처리 실패 | ❌ 유실 | ✅ 재시도 |
| 앱 재시작 | ❌ 유실 | ✅ 재처리 |
| DB 장애 | ❌ 유실 | ✅ 복구 후 재처리 |

#### 3.3.3 잔액 정합성 보장

현재 `MemberPointBalance`와 실제 잔액(`PointAccumulation` SUM) 간의 불일치가 발생할 수 있습니다.
Outbox Pattern 적용 시:

- 모든 이벤트가 순차적으로 처리됨
- 처리 실패 시 자동 재시도
- 장애 복구 후에도 미처리 이벤트 재처리
- **reconciliation 서비스 의존도 감소**

#### 3.3.4 감사 및 디버깅 용이

```sql
-- 특정 회원의 이벤트 처리 이력 조회
SELECT * FROM point_balance_outbox 
WHERE member_id = 12345 
ORDER BY created_at;

-- 실패한 이벤트 조회
SELECT * FROM point_balance_outbox 
WHERE status = 'FAILED';

-- 처리 지연 이벤트 조회
SELECT * FROM point_balance_outbox 
WHERE status = 'PENDING' 
AND created_at < NOW() - INTERVAL '5 MINUTES';
```

---

### 3.4 구현 방안

#### 3.4.1 Outbox 테이블 설계

```sql
CREATE TABLE point_balance_outbox (
    id              BIGINT PRIMARY KEY AUTO_INCREMENT,
    aggregate_type  VARCHAR(50) NOT NULL,      -- 'POINT_BALANCE'
    aggregate_id    BIGINT NOT NULL,           -- member_id
    event_type      VARCHAR(50) NOT NULL,      -- 'ACCUMULATED', 'USED', etc.
    payload         JSON NOT NULL,             -- 이벤트 데이터
    status          VARCHAR(20) NOT NULL,      -- 'PENDING', 'PROCESSED', 'FAILED'
    retry_count     INT DEFAULT 0,             -- 재시도 횟수
    max_retries     INT DEFAULT 3,             -- 최대 재시도 횟수
    error_message   TEXT,                      -- 실패 시 에러 메시지
    created_at      TIMESTAMP NOT NULL,
    processed_at    TIMESTAMP,
    
    INDEX idx_status_created (status, created_at),
    INDEX idx_aggregate (aggregate_type, aggregate_id)
);
```

#### 3.4.2 Outbox 엔티티

```kotlin
@Entity
@Table(name = "point_balance_outbox")
class PointBalanceOutboxEntity(
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    val id: Long? = null,
    
    @Column(name = "aggregate_type", nullable = false, length = 50)
    val aggregateType: String = "POINT_BALANCE",
    
    @Column(name = "aggregate_id", nullable = false)
    val aggregateId: Long,  // memberId
    
    @Column(name = "event_type", nullable = false, length = 50)
    val eventType: String,  // ACCUMULATED, USED, etc.
    
    @Column(name = "payload", nullable = false, columnDefinition = "JSON")
    val payload: String,  // JSON serialized event data
    
    @Enumerated(EnumType.STRING)
    @Column(name = "status", nullable = false, length = 20)
    var status: OutboxStatus = OutboxStatus.PENDING,
    
    @Column(name = "retry_count")
    var retryCount: Int = 0,
    
    @Column(name = "max_retries")
    val maxRetries: Int = 3,
    
    @Column(name = "error_message", columnDefinition = "TEXT")
    var errorMessage: String? = null,
    
    @Column(name = "created_at", nullable = false)
    val createdAt: LocalDateTime = LocalDateTime.now(),
    
    @Column(name = "processed_at")
    var processedAt: LocalDateTime? = null
)

enum class OutboxStatus {
    PENDING,    // 처리 대기
    PROCESSED,  // 처리 완료
    FAILED      // 처리 실패 (재시도 초과)
}
```

#### 3.4.3 서비스 수정 (이벤트를 Outbox에 저장)

```kotlin
@Service
class PointAccumulationService(
    private val pointAccumulationPersistencePort: PointAccumulationPersistencePort,
    private val pointBalanceOutboxPort: PointBalanceOutboxPort,  // 추가
    // ... 기타 의존성
) : PointAccumulationUseCase {

    @Transactional
    override fun accumulate(...): PointAccumulation {
        // 1. 포인트 적립 저장
        val savedAccumulation = pointAccumulationPersistencePort.save(accumulation)
        
        // 2. Outbox에 이벤트 저장 (동일 트랜잭션)
        pointBalanceOutboxPort.save(
            PointBalanceOutbox(
                aggregateId = memberId,
                eventType = "ACCUMULATED",
                payload = objectMapper.writeValueAsString(
                    mapOf(
                        "memberId" to memberId,
                        "amount" to amount.toLong(),
                        "pointKey" to savedAccumulation.pointKey
                    )
                )
            )
        )
        
        return savedAccumulation
    }
}
```

#### 3.4.4 Outbox 폴링 프로세서

```kotlin
@Component
class PointBalanceOutboxProcessor(
    private val outboxRepository: PointBalanceOutboxJpaRepository,
    private val memberPointBalancePersistencePort: MemberPointBalancePersistencePort,
    private val objectMapper: ObjectMapper
) {
    private val logger = LoggerFactory.getLogger(javaClass)
    
    /**
     * 주기적으로 미처리 이벤트를 폴링하여 처리
     */
    @Scheduled(fixedDelay = 1000)  // 1초마다 실행
    @Transactional
    fun processOutboxEvents() {
        val pendingEvents = outboxRepository.findByStatusOrderByCreatedAtAsc(
            OutboxStatus.PENDING,
            PageRequest.of(0, 100)
        )
        
        pendingEvents.forEach { event ->
            try {
                processEvent(event)
                event.status = OutboxStatus.PROCESSED
                event.processedAt = LocalDateTime.now()
            } catch (e: Exception) {
                handleFailure(event, e)
            }
            outboxRepository.save(event)
        }
    }
    
    private fun processEvent(event: PointBalanceOutboxEntity) {
        val payload = objectMapper.readTree(event.payload)
        val memberId = payload["memberId"].asLong()
        val amount = Money.of(payload["amount"].asLong())
        
        val balance = memberPointBalancePersistencePort.findByMemberIdWithLock(memberId)
            .orElseGet { MemberPointBalance(memberId) }
        
        when (event.eventType) {
            "ACCUMULATED" -> balance.addBalance(amount)
            "USED" -> balance.subtractBalance(amount)
            "USAGE_CANCELLED" -> balance.restoreBalance(amount)
            "ACCUMULATION_CANCELLED" -> balance.cancelAccumulation(amount)
            "EXPIRED" -> balance.expireBalance(amount)
        }
        
        memberPointBalancePersistencePort.save(balance)
        logger.info("Outbox 이벤트 처리 완료: id={}, type={}, memberId={}", 
            event.id, event.eventType, memberId)
    }
    
    private fun handleFailure(event: PointBalanceOutboxEntity, e: Exception) {
        event.retryCount++
        event.errorMessage = e.message
        
        if (event.retryCount >= event.maxRetries) {
            event.status = OutboxStatus.FAILED
            logger.error("Outbox 이벤트 처리 실패 (재시도 초과): id={}, error={}", 
                event.id, e.message)
            // TODO: 알림 발송 또는 DLQ 처리
        } else {
            logger.warn("Outbox 이벤트 처리 실패 (재시도 예정): id={}, retry={}/{}, error={}", 
                event.id, event.retryCount, event.maxRetries, e.message)
        }
    }
}
```

---

### 3.5 대안: Debezium CDC 방식

폴링 방식 대신 **Debezium CDC (Change Data Capture)**를 사용하면 더 효율적인 이벤트 처리가 가능합니다.

```
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│   포인트 서비스   │ ──→ │   Outbox 테이블  │ ──→ │    Debezium     │
│   (트랜잭션)     │     │   (MySQL)       │     │   Connector     │
└─────────────────┘     └─────────────────┘     └─────────────────┘
                                                         │
                                                         ↓
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│ MemberPointBalance│ ←── │   Consumer      │ ←── │     Kafka       │
│   업데이트       │     │   (이벤트 처리)  │     │   (이벤트 큐)   │
└─────────────────┘     └─────────────────┘     └─────────────────┘
```

**장점:**
- 폴링 오버헤드 없음
- 실시간 이벤트 처리
- 높은 처리량 지원

**단점:**
- 인프라 복잡도 증가 (Kafka, Debezium)
- 운영 비용 증가

---

### 3.6 도입 로드맵

#### Phase 1: 기반 구축 (1주)
- [ ] Outbox 테이블 설계 및 생성
- [ ] Outbox 엔티티 및 리포지토리 구현
- [ ] 기존 이벤트 발행 로직 수정 (Outbox 저장)

#### Phase 2: 폴링 프로세서 구현 (1주)
- [ ] Outbox 폴링 프로세서 구현
- [ ] 재시도 로직 구현
- [ ] 실패 이벤트 알림/DLQ 처리

#### Phase 3: 테스트 및 검증 (1주)
- [ ] 단위 테스트 및 통합 테스트 작성
- [ ] 장애 시나리오 테스트 (앱 재시작, DB 장애 등)
- [ ] 성능 테스트

#### Phase 4: 운영 적용 (1주)
- [ ] 스테이징 환경 배포 및 검증
- [ ] 운영 환경 배포
- [ ] 모니터링 대시보드 구축

---

### 3.7 의사결정 기준

#### Outbox Pattern 도입이 필요한 경우
- ✅ 이벤트 유실이 허용되지 않는 운영 환경
- ✅ 캐시 테이블(`MemberPointBalance`)의 정합성이 중요한 경우
- ✅ 장애 복구 및 이벤트 재처리가 필요한 경우
- ✅ 이벤트 처리 이력 감사가 필요한 경우

#### 현재 구현 유지가 적절한 경우
- ✅ 개발/테스트 환경
- ✅ 캐시 테이블의 일시적 불일치를 허용하는 경우
- ✅ reconciliation 서비스로 정합성 보정이 충분한 경우
- ✅ 인프라 구축 비용이 부담스러운 경우

---

## 4. 기타 개선 사항

### 4.1 캐싱 전략

**Redis 캐시 도입**
- 회원 잔액 조회 캐싱
- 설정 값 캐싱
- TTL 기반 자동 갱신

### 4.2 비동기 처리

**이벤트 기반 아키텍처**
- 포인트 적립/사용 이벤트 발행
- 이메일/푸시 알림 비동기 처리
- Kafka/RabbitMQ를 통한 메시지 큐잉

### 4.3 읽기 성능 최적화

**CQRS 패턴 적용**
- 쓰기 모델과 읽기 모델 분리
- 읽기 전용 복제본(Read Replica) 활용
- 조회 API 응답 시간 개선

---

## 5. 결론

현재 구현된 시스템은 단일 서버 환경에서 충분히 효과적입니다.
하지만 시스템이 성장하고 트래픽이 증가하면 분산 락 및 Outbox Pattern 도입을 검토해야 합니다.

### 현재 구현 상태

| 기능 | 현재 구현 | 향후 개선 |
|------|----------|----------|
| 동시성 제어 | JPA 비관적 락 | 분산 락 (Redis + Redisson) |
| 이벤트 처리 | Spring Event + @TransactionalEventListener | Transactional Outbox Pattern |
| 잔액 조회 | 캐시 테이블 (MemberPointBalance) | 유지 (Outbox로 정합성 강화) |

### 권장 도입 순서

**Phase 1-2 (현재):**
- 비관적 락 유지
- Spring Event 기반 비동기 처리
- Reconciliation 서비스로 정합성 보정

**Phase 3 (서버 스케일 아웃 시):**
- 분산 락 (Redis + Redisson) 도입
- Outbox Pattern 도입 (이벤트 유실 방지)

**Phase 4 (대규모 트래픽 시):**
- Debezium CDC 도입 (실시간 이벤트 처리)
- 메시지 큐 (Kafka) 도입

### 의사결정 기준

```
                    트래픽 증가
                         │
         ┌───────────────┼───────────────┐
         ▼               ▼               ▼
   단일 서버 유지    서버 스케일 아웃   대규모 분산 환경
         │               │               │
         ▼               ▼               ▼
   현재 구현 유지    분산 락 도입      CDC + Kafka
                    Outbox Pattern
```

---

**다음 문서**: 없음 (마지막 문서)
